{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1fc4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取資料集\n",
    "df = pd.read_csv('customer_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6915460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New set size:  3384 , ratio: 60.00%\n",
      "\n",
      "Origin set churn percentage:\n",
      " 0    0.831383\n",
      "1    0.168617\n",
      "Name: Churn, dtype: float64\n",
      "New set churn percentage:\n",
      " 0    0.831265\n",
      "1    0.168735\n",
      "Name: Churn, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#第1題\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "#取出60%\n",
    "split = StratifiedShuffleSplit(n_splits=1, train_size=0.6, random_state=15)\n",
    "\n",
    "for new_index, other_index in split.split(df, df['Churn']):\n",
    "    new_set = df.loc[new_index]\n",
    "    other_set = df.loc[other_index]\n",
    "\n",
    "\n",
    "#檢查分割結果:\n",
    "#可以看到經過stratified sampling的新資料(new_set)的資料比例為原本的60%\n",
    "#且新資料與原資料churn各類別分佈比例幾乎相同\n",
    "new_set_size = len(new_set)\n",
    "other_set_size = len(other_set)\n",
    "\n",
    "new_set_ratio = new_set_size / (new_set_size + other_set_size)\n",
    "\n",
    "print(\"New set size: \", new_set_size, \", ratio: {:.2%}\".format(new_set_ratio))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "origin_churn_percent = df['Churn'].value_counts(normalize=True)\n",
    "print(\"Origin set churn percentage:\\n\", origin_churn_percent)\n",
    "\n",
    "new_churn_percent = new_set['Churn'].value_counts(normalize=True)\n",
    "print(\"New set churn percentage:\\n\", new_churn_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d49e32f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2813\n",
      "1     571\n",
      "Name: Churn, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#第2題\n",
    "print(new_set['Churn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe3a7aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剩餘資料筆數： 3380\n",
      "\n",
      "每個欄位的空值個數：\n",
      "CustomerID                       0\n",
      "Churn                            0\n",
      "Tenure                         154\n",
      "PreferredLoginDevice             0\n",
      "CityTier                         0\n",
      "WarehouseToHome                141\n",
      "PreferredPaymentMode             0\n",
      "Gender                           0\n",
      "HourSpendOnApp                 148\n",
      "NumberOfDeviceRegistered         0\n",
      "PreferedOrderCat                 0\n",
      "SatisfactionScore                0\n",
      "MaritalStatus                    0\n",
      "NumberOfAddress                  0\n",
      "Complain                         0\n",
      "OrderAmountHikeFromlastYear    157\n",
      "CouponUsed                     140\n",
      "OrderCount                     160\n",
      "DaySinceLastOrder              189\n",
      "CashbackAmount                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#第3題 - 資料前處理(1)\n",
    "\n",
    "#刪除重複多餘的資料(保留一筆)\n",
    "new_set.drop_duplicates(subset=['CustomerID'], keep='first', inplace=True)\n",
    "print(\"剩餘資料筆數：\", len(new_set))\n",
    "\n",
    "#檢查空值\n",
    "print(\"\\n每個欄位的空值個數：\")\n",
    "print(new_set.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1c55171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第3題 - 資料前處理(2)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 使用平均數填補空值\n",
    "new_set['Tenure'].fillna(new_set['Tenure'].mean(), inplace=True)\n",
    "new_set['WarehouseToHome'].fillna(new_set['WarehouseToHome'].mean(), inplace=True)\n",
    "new_set['HourSpendOnApp'].fillna(new_set['HourSpendOnApp'].mean(), inplace=True)\n",
    "new_set['OrderAmountHikeFromlastYear'].fillna(new_set['OrderAmountHikeFromlastYear'].mean(), inplace=True)\n",
    "new_set['CouponUsed'].fillna(new_set['CouponUsed'].mean(), inplace=True)\n",
    "new_set['OrderCount'].fillna(new_set['OrderCount'].mean(), inplace=True)\n",
    "new_set['DaySinceLastOrder'].fillna(new_set['DaySinceLastOrder'].mean(), inplace=True)\n",
    "\n",
    "# Encoding\n",
    "le = LabelEncoder()\n",
    "new_set = pd.get_dummies(new_set, columns=['PreferredLoginDevice'])\n",
    "new_set = pd.get_dummies(new_set, columns=['PreferredPaymentMode'])\n",
    "new_set['Gender'] = le.fit_transform(new_set['Gender'])\n",
    "new_set = pd.get_dummies(new_set, columns=['PreferedOrderCat'])\n",
    "new_set = pd.get_dummies(new_set, columns=['MaritalStatus'])\n",
    "\n",
    "# 把預測欄位轉為nominal\n",
    "new_set['Churn'] = new_set['Churn'].astype('category')\n",
    "\n",
    "# 刪掉id欄\n",
    "new_set = new_set.drop('CustomerID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a41500d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第4題\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "X = new_set.drop(\"Churn\", axis=1)\n",
    "y = new_set[\"Churn\"]\n",
    "\n",
    "# 建立Logistic Regression模型\n",
    "lr = LogisticRegression(penalty='l1', C=1, solver='liblinear', max_iter=1000)\n",
    "lr_scores = cross_val_score(lr, X, y, cv=10,scoring='accuracy')\n",
    "\n",
    "# 建立SVM模型\n",
    "svm = SVC(kernel='rbf', C=5, gamma='auto')\n",
    "svm_scores = cross_val_score(svm, X, y, cv=10,scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e142d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression mean Accuracy: 0.8908284023668639\n",
      "SVM mean Accuracy: 0.9201183431952664\n"
     ]
    }
   ],
   "source": [
    "#第5題\n",
    "\n",
    "# 印出兩個模型各自交叉驗證10次的平均分數\n",
    "print(\"Logistic Regression mean Accuracy:\", lr_scores.mean())\n",
    "print(\"SVM mean Accuracy:\", svm_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "543ce517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 30次的平均Accuracy: 0.8897074856061771\n",
      "SVM 30次的平均Accuracy: 0.9227659410066447\n"
     ]
    }
   ],
   "source": [
    "#第6題\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "all_lr_scores = []\n",
    "all_svm_scores = []\n",
    "\n",
    "for i in range(30):\n",
    "    #重複第1題(取出60%)\n",
    "    # 這邊就不設定random_state，讓每次取得的60%資料都有所不同\n",
    "    split = StratifiedShuffleSplit(n_splits=1, train_size=0.6)\n",
    "    for new_index, other_index in split.split(df, df['Churn']):\n",
    "        new_set = df.loc[new_index]\n",
    "        other_set = df.loc[other_index]\n",
    "\n",
    "        \n",
    "    #重複第3題(資料前處理)\n",
    "    # 刪除重複多餘的資料(保留一筆)\n",
    "    new_set.drop_duplicates(subset=['CustomerID'], keep='first', inplace=True)   \n",
    "    # 使用平均數填補空值\n",
    "    new_set['Tenure'].fillna(new_set['Tenure'].mean(), inplace=True)\n",
    "    new_set['WarehouseToHome'].fillna(new_set['WarehouseToHome'].mean(), inplace=True)\n",
    "    new_set['HourSpendOnApp'].fillna(new_set['HourSpendOnApp'].mean(), inplace=True)\n",
    "    new_set['OrderAmountHikeFromlastYear'].fillna(new_set['OrderAmountHikeFromlastYear'].mean(), inplace=True)\n",
    "    new_set['CouponUsed'].fillna(new_set['CouponUsed'].mean(), inplace=True)\n",
    "    new_set['OrderCount'].fillna(new_set['OrderCount'].mean(), inplace=True)\n",
    "    new_set['DaySinceLastOrder'].fillna(new_set['DaySinceLastOrder'].mean(), inplace=True)\n",
    "    # Encoding\n",
    "    le = LabelEncoder()\n",
    "    new_set = pd.get_dummies(new_set, columns=['PreferredLoginDevice'])\n",
    "    new_set = pd.get_dummies(new_set, columns=['PreferredPaymentMode'])\n",
    "    new_set['Gender'] = le.fit_transform(new_set['Gender'])\n",
    "    new_set = pd.get_dummies(new_set, columns=['PreferedOrderCat'])\n",
    "    new_set = pd.get_dummies(new_set, columns=['MaritalStatus'])\n",
    "    # 把預測欄位轉為nominal\n",
    "    new_set['Churn'] = new_set['Churn'].astype('category')\n",
    "    # 刪掉id欄\n",
    "    new_set = new_set.drop('CustomerID', axis=1)\n",
    "    \n",
    "    #重複第4題(建立模型)\n",
    "    X = new_set.drop(\"Churn\", axis=1)\n",
    "    y = new_set[\"Churn\"]\n",
    "    # 建立Logistic Regression模型\n",
    "    lr = LogisticRegression(penalty='l1', C=1, solver='liblinear', max_iter=1000)\n",
    "    lr_scores = cross_val_score(lr, X, y, cv=10,scoring='accuracy')\n",
    "    all_lr_scores.append(lr_scores.mean())#將該次平均Accuracy紀錄\n",
    "    # 建立SVM模型\n",
    "    svm = SVC(kernel='rbf', C=5, gamma='auto')\n",
    "    svm_scores = cross_val_score(svm, X, y, cv=10,scoring='accuracy')\n",
    "    all_svm_scores.append(svm_scores.mean())#將該次平均Accuracy紀錄\n",
    "\n",
    "lr_average = sum(all_lr_scores) / len(all_lr_scores)\n",
    "svm_average = sum(all_svm_scores) / len(all_svm_scores)\n",
    "print(\"Logistic Regression 30次的平均Accuracy:\", lr_average)\n",
    "print(\"SVM 30次的平均Accuracy:\", svm_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4250ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR平均數： 0.8897074856061771\n",
      "SVM平均數： 0.9227659410066446\n",
      "LR標準差： 0.00370198693263988\n",
      "SVM標準差： 0.003440356920035281\n",
      "t-value： -39.430845644247476\n",
      "p-value： 9.547713390544607e-27\n"
     ]
    }
   ],
   "source": [
    "#第7題\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# 計算平均數\n",
    "lr_mean = np.mean(all_lr_scores)\n",
    "svm_mean = np.mean(all_svm_scores)\n",
    "\n",
    "# 計算標準差\n",
    "lr_std = np.std(all_lr_scores, ddof=1)\n",
    "svm_std = np.std(all_svm_scores, ddof=1)\n",
    "\n",
    "# 計算t-value和p-value\n",
    "t_value, p_value = stats.ttest_rel(all_lr_scores,all_svm_scores)\n",
    "\n",
    "# 輸出結果\n",
    "print(\"LR平均數：\", lr_mean)\n",
    "print(\"SVM平均數：\", svm_mean)\n",
    "print(\"LR標準差：\", lr_std)\n",
    "print(\"SVM標準差：\", svm_std)\n",
    "print(\"t-value：\", t_value)\n",
    "print(\"p-value：\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be060d",
   "metadata": {},
   "source": [
    "## 結論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c42191",
   "metadata": {},
   "source": [
    "若將顯著水準設置為0.05，以上得出的p-value小於此顯著水準，所以可以拒絕虛無假設，即表示兩模型的平均分數在統計上是存在差異的。\n",
    "\n",
    "所以結論就是在此次實驗中，SVM模型的表現優於Logistic Regression模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
